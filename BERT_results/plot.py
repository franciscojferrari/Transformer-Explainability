from matplotlib import pyplot as plt
import seaborn as sns
from torch.autograd import grad

sns.set_theme()
sns.set(font_scale=2)
num_token_range = list(range(5, 85, 5))
# token preference, macro-averaged F1-score
ours = [0.03297119155278638, 0.06092322462545505, 0.08396582252491494, 0.10336738131996127, 0.122730945238897, 0.13836658024576984, 0.15262779629099751, 0.16571801755961948, 0.17787014006969773, 0.18913713004248534, 0.19946829127172702, 0.20854177358039197, 0.21682762655019472, 0.22535940779167657, 0.23243333819452927, 0.23887383633982723]
chefer = [0.03583843981749835, 0.06463203413203165, 0.09039934694432565, 0.1123418850170835, 0.13090225925226667, 0.1483890598869277, 0.1631209681511303, 0.17594520962045446, 0.18873729028741779, 0.19882552850626145, 0.20887074554877663, 0.21801386657372251, 0.22762035250100332, 0.23515804769919793, 0.2426316935369942, 0.25005390369750885]
partial_lrp = [0.033938859750878396, 0.06141008635018109, 0.08429931201477159, 0.1043098782839015, 0.12207408094556203, 0.13934691814586156, 0.1548874363742686, 0.16936175012784244, 0.18045236658049785, 0.1910013902482886, 0.20112700682132037, 0.21134168902266104, 0.2207388496538799, 0.22842757087383084, 0.2359809176488736, 0.24253673248085972]
rollout = [0.0032863435099534967, 0.008713420279915339, 0.015013350987933194, 0.023030102408312048, 0.029986813033643887, 0.038417848483761154, 0.04729772560421312, 0.055736047714096154, 0.06284091612839973, 0.06965831868640249, 0.07667532795246784, 0.08258992749626005, 0.08901670620239864, 0.09597152287965313, 0.10256162438677459, 0.10879385489842264]
gradcam = [0.017844978544946558, 0.03263865174336229, 0.0458218537807952, 0.05917921596007014, 0.07160352814060557, 0.08240054527545587, 0.09277218851591408, 0.10224789925221593, 0.11162248960674427, 0.11956252289674983, 0.1260683061778357, 0.13261316762732897, 0.13915368903116548, 0.1436116092454343, 0.14884935753267717, 0.15359969371128895]
last_attn = [0.030897898599149504, 0.054895086681738314, 0.07390737092227069, 0.08970364604737353, 0.1025167466213224, 0.11569440980987734, 0.12774715803763356, 0.14007296661633334, 0.15128804978918223, 0.15965659765850368, 0.17004902872182356, 0.18034734870419344, 0.18854552504805266, 0.19685418529127438, 0.20423422828862006, 0.21294725853290078]
lrp = [0.0121907930979934, 0.024188778588375523, 0.03315844914875055, 0.043893453753583195, 0.05202103410870659, 0.06004360119635506, 0.06701760854636109, 0.07499920484771377, 0.08191901667383215, 0.08843883298811284, 0.09470231194879825, 0.10033496082819326, 0.10561885330718933, 0.11050128105770832, 0.11536660235985254, 0.12163973040939814]
ax = sns.lineplot(x=num_token_range, y=ours, label="Ours")
ax = sns.lineplot(x=num_token_range, y=chefer, label="Chefer et al.")
ax = sns.lineplot(x=num_token_range, y=partial_lrp, label="Partial LRP")
ax = sns.lineplot(x=num_token_range, y=rollout, label="Rollout")
ax = sns.lineplot(x=num_token_range, y=gradcam, label="GradCAM")
ax = sns.lineplot(x=num_token_range, y=last_attn, label="Raw Attention")
ax = sns.lineplot(x=num_token_range, y=lrp, label="LRP")

ax.set( xlabel = "# tokens", ylabel = "Macro-F1", xlim=(5, 80))
ax.set_title("")

# plt.tight_layout()
plt.show()
