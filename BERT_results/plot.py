from matplotlib import pyplot as plt
import seaborn as sns
from torch.autograd import grad

sns.set_theme()
sns.set(font_scale=2, rc={"lines.linewidth": 2})
num_token_range = list(range(5, 85, 5))
# token preference, macro-averaged F1-score
ours = [0.03297119155278638, 0.06092322462545505, 0.08396582252491494, 0.10336738131996127, 0.122730945238897, 0.13836658024576984, 0.15262779629099751, 0.16571801755961948, 0.17787014006969773, 0.18913713004248534, 0.19946829127172702, 0.20854177358039197, 0.21682762655019472, 0.22535940779167657, 0.23243333819452927, 0.23887383633982723]
ours_no_clone = [0.03278199826467432, 0.06151445948270044, 0.08429455661253683, 0.10360873909233537, 0.12375052248218757, 0.1384123075351252, 0.15283760036446944, 0.1660578741799284, 0.17779242881404084, 0.1894611970087585, 0.1995956352870319, 0.20931380656760776, 0.21794315883616386, 0.22645635801876787, 0.23408875979641736, 0.24024888744290834]
ours_idk = [0.03278199826467431, 0.061514459482700484, 0.08429455661253682, 0.10360873909233546, 0.1237505224821876, 0.13841230753512512, 0.15283760036446944, 0.1660578741799284, 0.17779242881404092, 0.18946119700875855, 0.1995956352870318, 0.20931380656760762, 0.2179431588361637, 0.2264563580187678, 0.23408875979641755, 0.2402488874429083]
ours_linear_eps_rule = [0.024677454889317108, 0.04667580260006282, 0.06528330363376492, 0.08292938730547174, 0.0987168448525546, 0.11290041564000801, 0.12524677334599285, 0.13679964812783157, 0.14813577913463205, 0.1590313104813123, 0.16924776458474336, 0.17805440987671575, 0.1854888204725341, 0.19261931076036673, 0.2003010278641957, 0.20741212244915755]
ours_alpha_0_9_beta_0_1 = [0.03302122501425018, 0.06101973641997168, 0.08411111542040821, 0.1034458462055407, 0.12283819815511843, 0.1380271338224777, 0.15216200217584924, 0.16570461271415277, 0.17777964548348432, 0.1890301934424544, 0.19939577707696302, 0.20873336546246324, 0.21712316361551368, 0.22600664606411086, 0.23334137882421713, 0.23969562167533892]
ours_alpha_0_5_beta_0_5 = [0.03205685033117056, 0.05896452411594227, 0.0810781405996938, 0.10144326457011281, 0.11991356407210553, 0.1358050768315844, 0.1493671371718177, 0.16280767281008746, 0.17526254980983777, 0.18652036599343155, 0.19660980192320587, 0.20625349222165726, 0.21496574060786264, 0.2232606794205797, 0.23037256174612275, 0.23653629415573238]
chefer = [0.03583843981749835, 0.06463203413203165, 0.09039934694432565, 0.1123418850170835, 0.13090225925226667, 0.1483890598869277, 0.1631209681511303, 0.17594520962045446, 0.18873729028741779, 0.19882552850626145, 0.20887074554877663, 0.21801386657372251, 0.22762035250100332, 0.23515804769919793, 0.2426316935369942, 0.25005390369750885]
partial_lrp = [0.033938859750878396, 0.06141008635018109, 0.08429931201477159, 0.1043098782839015, 0.12207408094556203, 0.13934691814586156, 0.1548874363742686, 0.16936175012784244, 0.18045236658049785, 0.1910013902482886, 0.20112700682132037, 0.21134168902266104, 0.2207388496538799, 0.22842757087383084, 0.2359809176488736, 0.24253673248085972]
rollout = [0.0032863435099534967, 0.008713420279915339, 0.015013350987933194, 0.023030102408312048, 0.029986813033643887, 0.038417848483761154, 0.04729772560421312, 0.055736047714096154, 0.06284091612839973, 0.06965831868640249, 0.07667532795246784, 0.08258992749626005, 0.08901670620239864, 0.09597152287965313, 0.10256162438677459, 0.10879385489842264]
gradcam = [0.017844978544946558, 0.03263865174336229, 0.0458218537807952, 0.05917921596007014, 0.07160352814060557, 0.08240054527545587, 0.09277218851591408, 0.10224789925221593, 0.11162248960674427, 0.11956252289674983, 0.1260683061778357, 0.13261316762732897, 0.13915368903116548, 0.1436116092454343, 0.14884935753267717, 0.15359969371128895]
last_attn = [0.030897898599149504, 0.054895086681738314, 0.07390737092227069, 0.08970364604737353, 0.1025167466213224, 0.11569440980987734, 0.12774715803763356, 0.14007296661633334, 0.15128804978918223, 0.15965659765850368, 0.17004902872182356, 0.18034734870419344, 0.18854552504805266, 0.19685418529127438, 0.20423422828862006, 0.21294725853290078]
lrp = [0.0121907930979934, 0.024188778588375523, 0.03315844914875055, 0.043893453753583195, 0.05202103410870659, 0.06004360119635506, 0.06701760854636109, 0.07499920484771377, 0.08191901667383215, 0.08843883298811284, 0.09470231194879825, 0.10033496082819326, 0.10561885330718933, 0.11050128105770832, 0.11536660235985254, 0.12163973040939814]
ax = sns.lineplot(x=num_token_range, y=ours_idk, label="Ours")
ax = sns.lineplot(x=num_token_range, y=ours_linear_eps_rule, label=r"Ours w. $\epsilon$-rule")
ax = sns.lineplot(x=num_token_range, y=ours_alpha_0_9_beta_0_1, label=r"Ours $\alpha=0.9$ $\beta=0.1$")
ax = sns.lineplot(x=num_token_range, y=ours_alpha_0_5_beta_0_5, label=r"Ours $\alpha=0.5$ $\beta=0.5$")
ax = sns.lineplot(x=num_token_range, y=chefer, label="Chefer et al.")
ax = sns.lineplot(x=num_token_range, y=partial_lrp, label="Partial LRP")
ax = sns.lineplot(x=num_token_range, y=rollout, label="Rollout")
ax = sns.lineplot(x=num_token_range, y=gradcam, label="GradCAM")
ax = sns.lineplot(x=num_token_range, y=last_attn, label="Raw Attention")
ax = sns.lineplot(x=num_token_range, y=lrp, label="LRP")

ax.set( xlabel = "# tokens", ylabel = "Macro-F1", xlim=(5, 80))
ax.set_title("")

# plt.tight_layout()
plt.show()
