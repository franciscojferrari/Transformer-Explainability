# Transformer-Explainability
Implementation of the explainability method for Transfomer architecture proposed in [Transformer Interpretability Beyond Attention Visualization](https://arxiv.org/abs/2012.09838). 

We find a closed form for relevance propagation for each layer type, which reduces computational costs by 38% and incrementally improves performance. Analysis of the method and results available in our project report.

This project was part of [DD2412 - Deep Learning, Advanced](https://www.kth.se/student/kurser/kurs/DD2412?l=en) at KTH. 

Please refere to the ViT or BERT folder to run the different experiements
